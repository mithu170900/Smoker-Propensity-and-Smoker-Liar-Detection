{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(grid, X, y, cv=3, scorer='roc_auc', n_jobs=-1):\n",
    "    rf = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = grid, \n",
    "                          cv = cv, n_jobs = n_jobs, scoring = scorer)\n",
    "    grid_search.fit(X,y)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    return grid_search, best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    pickle.dump(model, open(name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_pred(best_estimator, X_test):\n",
    "    probabilities = best_estimator.predict_proba(X_test)[:,1]\n",
    "    predictions = best_estimator.predict(X_test)\n",
    "    return probabilities, predictions\n",
    "\n",
    "def get_prob_distribution(probs, preds):\n",
    "    print(pd.Series(preds).value_counts())\n",
    "    pd.Series(probs).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_score(probs, actual):\n",
    "    auc_score = roc_auc_score(actual, probs)\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def establishing_threshold_with_accuracy(probs, threshold, actual):\n",
    "    threshold_predictions = pd.Series(np.where(probs > threshold, 1, 0))\n",
    "    accuracy_rate = accuracy_score(actual, threshold_predictions)\n",
    "    return threshold, threshold_predictions, accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(actual, threshold_preds):\n",
    "    return classification_report(actual, threshold_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(model, columns, n=10):\n",
    "    feature_importance = pd.Series(model.feature_importances_, index=columns)\n",
    "    ax = feature_importance.nlargest(n).sort_values(ascending=True).plot.barh()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, preds):\n",
    "    matrix = confusion_matrix(actual, predicted)\n",
    "    x_axis_labels = sorted(predicted.unique().tolist())\n",
    "    y_axis_labels = sorted(actual.unique().tolist())\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(matrix, cmap=\"Blues\", cbar=False, annot=True, fmt=',',\n",
    "                linewidths=1, linecolor='grey', square=True)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(x_axis_labels)\n",
    "    ax.yaxis.set_ticklabels(y_axis_labels)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
